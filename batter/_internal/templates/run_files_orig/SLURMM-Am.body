scontrol show job $SLURM_JOB_ID
# start time
echo "Job started at $(date)"

OVERWRITE=${OVERWRITE:-0}  # Default to 0 if not set
ONLY_EQ=${ONLY_EQ:-0}  # Default to 0 if not set
echo "OVERWRITE: $OVERWRITE"
echo "ONLY_EQ: $ONLY_EQ"

# Set a path for the retry count file on a shared filesystem.
# Change this path as needed.
ATTEMPT_FILE="job_attempt.txt"

# Initialize the attempt count file if it doesn't exist.
if [ ! -f "$ATTEMPT_FILE" ]; then
    echo "1" > "$ATTEMPT_FILE"
fi

# Read the current retry count.
RETRY_COUNT=$(cat "$ATTEMPT_FILE")
echo "Current attempt: $RETRY_COUNT"

# Define the maximum number of attempts.
MAX_ATTEMPTS=5

# Run the simulation
echo "Attempt $RETRY_COUNT to run simulations..."
OVERWRITE=$OVERWRITE ONLY_EQ=$ONLY_EQ RETRY_COUNT=$RETRY_COUNT source run-local.bash 2>&1 | tee run.log
error_code=${PIPESTATUS[0]}  # Capture the exit status of 'source run-local.bash'

if [ $error_code -eq 0 ]; then
    echo "Simulation completed successfully on attempt $RETRY_COUNT."
else
    echo "Simulation failed with error code $error_code on attempt $RETRY_COUNT."

    if [ "$RETRY_COUNT" -lt "$MAX_ATTEMPTS" ]; then
        NEXT_RETRY=$((RETRY_COUNT + 1))
        echo "$NEXT_RETRY" > "$ATTEMPT_FILE"
        echo "Requeuing the job for attempt $NEXT_RETRY..."

        sleep 30

        bad_node="${SLURMD_NODENAME:-$(hostname -s)}"
        echo "Bad node: ${bad_node:-<empty>}"

        if [ -n "$bad_node" ]; then
            echo "Requeuehold -> wait PENDING -> update exclusion -> release"
            scontrol requeuehold "$SLURM_JOB_ID"

            # wait for PENDING (held) so update sticks
            pending_ok=0
            for _ in {1..60}; do
                state=$(scontrol show job "$SLURM_JOB_ID" | awk -F= '/JobState=/{print $2}' | awk '{print $1; exit}')
                if [[ "$state" == "PENDING" ]]; then
                    pending_ok=1
                    break
                fi
                sleep 1
            done

            if [[ "$pending_ok" -ne 1 ]]; then
                echo "WARNING: job did not reach PENDING after requeuehold (state=$state). Requeueing without exclusion."
                scontrol requeue "$SLURM_JOB_ID"
                exit 0
            fi

            # NOW read existing ExcNodeList (while PENDING) and merge
            cur_exc_nodes=$(
                scontrol show job "$SLURM_JOB_ID" |
                awk -F= '/ExcNodeList=/{print $2}' |
                awk '{print $1; exit}'
            )

            if [[ -n "$cur_exc_nodes" && "$cur_exc_nodes" != "(null)" && "$cur_exc_nodes" != "None" ]]; then
                exc_nodes=$(printf "%s,%s\n" "$cur_exc_nodes" "$bad_node" | tr ',' '\n' | awk 'NF' | sort -u | paste -sd, -)
            else
                exc_nodes="$bad_node"
            fi

            echo "Excluding: $exc_nodes"
            if ! out=$(scontrol update JobId="$SLURM_JOB_ID" ExcNodeList="$exc_nodes" 2>&1); then
                echo "scontrol update FAILED: $out"
            else
                echo "scontrol update OK: $out"
            fi

            # Confirm it stuck (should show ExcNodeList=... not (null))
            scontrol show job "$SLURM_JOB_ID" | egrep -i 'JobState=|Reason=|BatchHost=|NodeList=|ExcNodeList='

            scontrol release "$SLURM_JOB_ID"
            exit 0
        fi

        # If bad_node is empty, just requeue
        scontrol requeue "$SLURM_JOB_ID"
        exit 0
    else
        echo "FAILED" > FAILED
        echo "Maximum attempts ($MAX_ATTEMPTS) reached. Simulation failed."
        exit 1
    fi
fi

if [ $ONLY_EQ -eq 1 ]; then
    echo "ONLY_EQ is set to 1. Simulation is complete."
    if [[ ! -f EQ_FINISHED ]]; then
        echo "EQ failed for some unknown reason."
        scontrol requeue $SLURM_JOB_ID
        exit 0
    fi
    echo "Job completed at $(date)"
    exit 0
fi
if [[ ! -f FINISHED ]]; then
    echo "Simulation is not complete."
    scontrol requeue $SLURM_JOB_ID
    exit 0
fi
echo "Job completed at $(date)"
exit 0
