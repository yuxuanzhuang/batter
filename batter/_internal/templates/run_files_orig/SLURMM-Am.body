scontrol show job $SLURM_JOB_ID
# start time
echo "Job started at $(date)"

OVERWRITE=${OVERWRITE:-0}  # Default to 0 if not set
ONLY_EQ=${ONLY_EQ:-0}  # Default to 0 if not set
echo "OVERWRITE: $OVERWRITE"
echo "ONLY_EQ: $ONLY_EQ"

# Set a path for the retry count file on a shared filesystem.
# Change this path as needed.
ATTEMPT_FILE="job_attempt.txt"

# Initialize the attempt count file if it doesn't exist.
if [ ! -f "$ATTEMPT_FILE" ]; then
    echo "1" > "$ATTEMPT_FILE"
fi

# Read the current retry count.
RETRY_COUNT=$(cat "$ATTEMPT_FILE")
echo "Current attempt: $RETRY_COUNT"

# Define the maximum number of attempts.
MAX_ATTEMPTS=5

# Run the simulation
echo "Attempt $RETRY_COUNT to run simulations..."
OVERWRITE=$OVERWRITE ONLY_EQ=$ONLY_EQ RETRY_COUNT=$RETRY_COUNT source run-local.bash 2>&1 | tee run.log
error_code=${PIPESTATUS[0]}  # Capture the exit status of 'source run-local.bash'

if [ $error_code -eq 0 ]; then
    echo "Simulation completed successfully on attempt $RETRY_COUNT."
else
    echo "Simulation failed with error code $error_code on attempt $RETRY_COUNT."

    if [ "$RETRY_COUNT" -lt "$MAX_ATTEMPTS" ]; then
        NEXT_RETRY=$((RETRY_COUNT + 1))
        echo "$NEXT_RETRY" > "$ATTEMPT_FILE"
        echo "Requeuing the job for attempt $NEXT_RETRY..."

        # Grab bad node early (before any delays / teardown)
        bad_node="${SLURMD_NODENAME:-$(hostname -s)}"
        echo "Bad node: ${bad_node:-<empty>}"

        # Small delay to let filesystem/logs settle (optional)
        sleep 5

        if [ -n "$bad_node" ]; then
            # Try to read existing exclusions, but don't fail if job is disappearing
            cur_exc_nodes=$(
                scontrol show job "$SLURM_JOB_ID" 2>/dev/null |
                awk -F= '/ExcNodeList=/{print $2}' |
                awk '{print $1; exit}'
            )

            if [[ -n "$cur_exc_nodes" && "$cur_exc_nodes" != "(null)" && "$cur_exc_nodes" != "None" ]]; then
                exc_nodes=$(printf "%s,%s\n" "$cur_exc_nodes" "$bad_node" | tr ',' '\n' | awk 'NF' | sort -u | paste -sd, -)
            else
                exc_nodes="$bad_node"
            fi

            echo "Excluding: $exc_nodes"

            export_vars="OVERWRITE=${OVERWRITE},ONLY_EQ=${ONLY_EQ},RETRY_COUNT=${NEXT_RETRY}"
            if [[ -n "${SKIP_WINDOW_EQ-}" ]]; then
                export_vars="${export_vars},SKIP_WINDOW_EQ=${SKIP_WINDOW_EQ}"
            fi

            if [[ -z "${RESUBMIT_SCRIPT-}" ]]; then
                if [[ -n "${SLURM_SUBMIT_DIR-}" && -f "${SLURM_SUBMIT_DIR}/SLURMM-run" ]]; then
                    resubmit_script="${SLURM_SUBMIT_DIR}/SLURMM-run"
                else
                    resubmit_script="$0"
                fi
            else
                resubmit_script="${RESUBMIT_SCRIPT}"
            fi

            # Capture partition/QOS/account/time from current job (fallback to env if missing)
            cur_part=$(scontrol show job "$SLURM_JOB_ID" 2>/dev/null | awk -F= '/Partition=/{print $2}' | awk '{print $1; exit}')
            cur_qos=$(scontrol show job "$SLURM_JOB_ID" 2>/dev/null | awk -F= '/QOS=/{print $2}' | awk '{print $1; exit}')
            cur_acct=$(scontrol show job "$SLURM_JOB_ID" 2>/dev/null | awk -F= '/Account=/{print $2}' | awk '{print $1; exit}')
            cur_time=$(scontrol show job "$SLURM_JOB_ID" 2>/dev/null | awk -F= '/TimeLimit=/{print $2}' | awk '{print $1; exit}')
            [[ -z "${cur_part}" || "${cur_part}" == "(null)" ]] && cur_part="${SLURM_JOB_PARTITION-}"
            [[ -z "${cur_qos}" || "${cur_qos}" == "(null)" ]] && cur_qos="${SLURM_JOB_QOS-}"
            [[ -z "${cur_acct}" || "${cur_acct}" == "(null)" ]] && cur_acct="${SLURM_JOB_ACCOUNT-}"
            [[ -z "${cur_time}" || "${cur_time}" == "(null)" ]] && cur_time="${SLURM_TIMELIMIT-}"

            sbatch_args=(--parsable --job-name="${SLURM_JOB_NAME}" --export=ALL,"$export_vars" --exclude="$exc_nodes")
            [[ -n "${cur_part}" ]] && sbatch_args+=(-p "${cur_part}")
            [[ -n "${cur_qos}" ]] && sbatch_args+=(--qos "${cur_qos}")
            [[ -n "${cur_acct}" ]] && sbatch_args+=(--account "${cur_acct}")
            [[ -n "${cur_time}" ]] && sbatch_args+=(-t "${cur_time}")

            # Submit retry as a *new job* with node exclusion
            submit_out=$(sbatch "${sbatch_args[@]}" "$resubmit_script" 2>&1)
            rc=$?

            if [[ $rc -eq 0 && -n "$submit_out" ]]; then
                echo "Resubmitted as JobId=$submit_out"
                echo "$submit_out" > RESUBMITTED_JOBID
                echo "$submit_out" > JOBID

                echo "Canceling current job $SLURM_JOB_ID"
                scancel "$SLURM_JOB_ID"
                exit 0
            fi

            echo "sbatch resubmit FAILED (rc=$rc): $submit_out"
            echo "Falling back to scontrol requeue (no exclusion guarantee)."
            scontrol requeue "$SLURM_JOB_ID" || true
            exit 0
        fi

        # If bad_node is empty, just requeue
        scontrol requeue "$SLURM_JOB_ID" || true
        exit 0
    else
        echo "FAILED" > FAILED
        echo "Maximum attempts ($MAX_ATTEMPTS) reached. Simulation failed."
        exit 1
    fi
fi

if [ $ONLY_EQ -eq 1 ]; then
    echo "ONLY_EQ is set to 1. Simulation is complete."
    if [[ ! -f EQ_FINISHED ]]; then
        echo "EQ failed for some unknown reason."
        scontrol requeue $SLURM_JOB_ID
        exit 0
    fi
    echo "Job completed at $(date)"
    exit 0
fi
if [[ ! -f FINISHED ]]; then
    echo "Simulation is not complete."
    scontrol requeue $SLURM_JOB_ID
    exit 0
fi
echo "Job completed at $(date)"
exit 0
